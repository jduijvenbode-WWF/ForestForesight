% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ff_train.R
\name{ff_train}
\alias{ff_train}
\title{Train an XGBoost Model for ForestForesight}
\usage{
ff_train(
  train_matrix,
  validation_matrix = NA,
  nrounds = 200,
  eta = 0.1,
  max_depth = 5,
  subsample = 0.75,
  eval_metric = "aucpr",
  early_stopping_rounds = 10,
  gamma = NULL,
  maximize = NULL,
  min_child_weight = 1,
  verbose = F,
  xgb_model = NULL,
  modelfilename = NULL,
  objective = "binary:logistic"
)
}
\arguments{
\item{train_matrix}{An xgb.DMatrix object or a list containing 'features' and 'label' for training.}

\item{validation_matrix}{An xgb.DMatrix object or a list containing 'features' and 'label' for validation. Default is NA.}

\item{nrounds}{Number of boosting rounds. Default is 200.}

\item{eta}{Learning rate. Default is 0.1.}

\item{max_depth}{Maximum tree depth. Default is 5.}

\item{subsample}{Subsample ratio of the training instances. Default is 0.75.}

\item{eval_metric}{Evaluation metric. Default is "aucpr". Can be a custom evaluation metric.}

\item{early_stopping_rounds}{Number of rounds for early stopping. Default is 10.}

\item{gamma}{Minimum loss reduction required to make a further partition. Default is NULL.}

\item{maximize}{Boolean indicating whether to maximize the evaluation metric. Required for custom metrics.}

\item{min_child_weight}{Minimum sum of instance weight needed in a child. Default is 1.}

\item{verbose}{Boolean indicating whether to display training progress. Default is FALSE.}

\item{xgb_model}{Previously trained model to continue training from. Can be an "xgb.Booster" object, raw data, or a file name. Default is NULL.}

\item{modelfilename}{String specifying where to save the model. Should end with ".model" extension.}

\item{objective}{Learning objective. Default is "binary:logistic".}
}
\value{
A trained XGBoost model (xgb.Booster object).
}
\description{
This function trains an XGBoost model with optimized default parameters derived from worldwide data analysis.
}
\examples{
\dontrun{
# Prepare your data
train_data <- list(features = matrix(runif(1000), ncol = 10),
                   label = sample(0:1, 100, replace = TRUE))

# Train the model
model <- ff_train(train_matrix = train_data,
                  nrounds = 100,
                  eta = 0.05,
                  max_depth = 6,
                  modelfilename = "forest_model.model",
                  features = colnames(train_data$features))
}

}
\references{
Jonas van Duijvenbode (2023)
Zillah Calle (2023)
}
\seealso{
\code{\link{ff_prep}} for preparing data for this function
\code{\link{ff_predict}} for making predictions using the trained model
}
\keyword{forestry}
\keyword{machine-learning}
\keyword{xgboost}
